{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088b5478-a4eb-4574-bcd3-9c2d0bab1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803eca9-c3bd-4376-ad6c-0a8d46d96274",
   "metadata": {},
   "source": [
    "### 1. use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy:\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "\n",
    "#### The site is down and we are going to use csv files instead of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64eb67a4-4c15-468f-803c-d6c212e091df",
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error no host given>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_249/1388865493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# create dataframes for general tweets and keyword tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgen_tweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mkey_tweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/urllib/request.py\u001b[0m in \u001b[0;36mdo_request_\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no host given'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# POST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error no host given>"
     ]
    }
   ],
   "source": [
    "# create a dataframe with the two file instead of the urls\n",
    "\n",
    "# define paths\n",
    "gen_path = Path('../data/general-tweets.csv') \n",
    "\n",
    "key_path = Path('../data/keyword-tweets.csv') \n",
    "\n",
    "# create dataframes for general tweets and keyword tweets\n",
    "gen_tweets_df = pd.read_csv(gen_path)\n",
    "\n",
    "key_tweets_df = pd.read_csv(key_path)\n",
    "\n",
    "# show head of both dfs\n",
    "display(gen_tweets_df.head())\n",
    "display(key_tweets_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75795b-f134-4efa-9372-b5d7932f6caf",
   "metadata": {},
   "source": [
    "### 2. concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet <span style=\"color:red\" float:right>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad5b3a-3623-4ab5-ad71-ac5d799e3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column names \n",
    "col_names = 'Sentiment', 'Tweet'\n",
    "\n",
    "# concatenate dfs on rows\n",
    "sentiment_tweet_df = pd.concat([gen_tweets_df, key_tweets_df], axis=0)\n",
    "\n",
    "# rename columns\n",
    "sentiment_tweet_df.columns = col_names\n",
    "\n",
    "# review dataframe\n",
    "sentiment_tweet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b1ae2-e9dc-491d-8634-f4ef9ff298d9",
   "metadata": {},
   "source": [
    "**We are concatenating on the rows axis and not the columns axis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecfe6f1-e2cc-4c07-9f54-bdef68b33c9c",
   "metadata": {},
   "source": [
    "### 3. replace sentiment labels 'POLIT': 1, 'NOT': 0; <span style=\"color:red\" float:right>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde21c73-f3dc-450e-ba53-13ce630fdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace labels with 1 or 0\n",
    "sentiment_tweet_df.replace('NOT', 0, inplace=True)\n",
    "sentiment_tweet_df.replace('POLIT', 1, inplace=True)\n",
    "\n",
    "# review dataframe\n",
    "display(sentiment_tweet_df)\n",
    "\n",
    "# create copy for testing\n",
    "sentiment_copy = sentiment_tweet_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2fa6d-e0f2-4c0b-a8a2-7c6c2f0c910f",
   "metadata": {},
   "source": [
    "**Replacing the sentiment values into computer interpretable values was simple with the .replace method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c5eca-0d3e-46b7-819b-6c682c686127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dtypes\n",
    "sentiment_copy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a1541-5b6d-4746-b327-041818a2ba01",
   "metadata": {},
   "source": [
    "### 4. clean the tweets\n",
    "   1. remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "   2. remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "   3. **replace** (not remove) all punctuation marks with a space (\" \")\n",
    "   4. **replace** all numbers with a space\n",
    "   5. **replace** all non ascii characters with a space\n",
    "   7. convert all characters to lowercase\n",
    "   8. strip extra whitespaces\n",
    "   9. lemmatize tokens\n",
    "   9. No need to remove stopwords because TfidfVectorizer will take care of that\n",
    "<br/><span style=\"color:red\" float:right>[9 point]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bd767-c06a-4d86-a5fd-d9f32c051847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        # step 1 remove entire tokens starting with ampersand\n",
    "        if step == 'remove_amp':\n",
    "            text = ' '.join([x for x in text.split() if not x.startswith(\"@\")])\n",
    "        # step 2 remove entire tokens starting with http    \n",
    "        elif step == 'remove_http':\n",
    "            text = ' '.join([x for x in text.split() if not x.startswith(\"http\")])\n",
    "        # step 3 replace punctuation with space     \n",
    "        elif step == 'replace_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            for char in text:\n",
    "                if char in punct_exclude:\n",
    "                    text = text.replace(char, ' ')\n",
    "        # step 4 replace numbers    \n",
    "        elif step == 'replace_numbers':\n",
    "            for char in text:\n",
    "                try:\n",
    "                    if char.isdigit():\n",
    "                        text = text.replace(char, ' ')\n",
    "                except:\n",
    "                    pass\n",
    "        # step 5 replace non ascii characters with space    \n",
    "        elif step == 'replace_non_ascii':\n",
    "            for char in text:\n",
    "                if ord(char) >= 128:\n",
    "                    text = text.replace(char, ' ')\n",
    "        # step 6 turn all text to lowercase    \n",
    "        elif step == 'lower_case':\n",
    "            text = text.lower()\n",
    "        # step 7 strip the white space    \n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        # step 8 lemmatizze the words into their stems    \n",
    "        elif step == 'lemmatize':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    # finally return the processed text        \n",
    "    return text\n",
    "\n",
    "# Outline the steps\n",
    "step_list = ['remove_amp', 'remove_http', 'replace_punctuation', 'replace_numbers',\n",
    "            'replace_non_ascii', 'lower_case', 'strip_whitespace', 'lemmatize']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754064d-6915-4d0d-97ba-c2b528bb8cd4",
   "metadata": {},
   "source": [
    "**While the instructions say to replace punctuation, numbers and non ascii characters with a space, when putting ' '.join(...) instead of ''.join(... I continued to get nonsense strings that would eventually error out as they were all stop words when trying to vectorize. However, with a try and except block I was able to meet the requirement that asks for replacing the characters with a space instead of just taking out the character completely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d291c-2b22-47bb-994b-8859bdec464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a test string\n",
    "test_string = \"@kajhds // ci **87 http//iciidnnwh asdkk't skkxi &dkic k,idi to for by  éè    UIOPHHGVG\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e7089-dd88-4727-abc8-896d11836e29",
   "metadata": {},
   "source": [
    "**I created a test string to try out the steps prior to applying the function to the entire dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c87d017f-d90b-457f-b35b-fa2d933a1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ci asdkk t skkxi dkic k idi to for by uiophhgvg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on test string\n",
    "clean_text = clean(test_string, step_list)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ace45-b5fe-4dd0-b4a1-0577833a27f3",
   "metadata": {},
   "source": [
    "**It looks like the cleaning worked on the test string created above so I will go ahead and use it on the entire df. I wanted to test out one line of text to make troubleshooting simpler. I also had tried to go about this function in a different, unique way as opposed to recycling the one from class. I managed to get it to wok on the single text line, but when applied to entire dataframe, Series errors and others kept occurring and so I decided to just re-use the one from the professors code with a few minor changes to meet the requirements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1887b5f-76b4-4d12-aad8-fafb9431acd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Bumping dj sefs mixtape nowww this is my music...</td>\n",
       "      <td>bumping dj sefs mixtape nowww this is my music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>#ieroween THE STORY OF IEROWEEN! THE VIDEO -&gt;&gt;...</td>\n",
       "      <td>ieroween the story of ieroween the video just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>trick or treating at the mall today; ZOO! last...</td>\n",
       "      <td>trick or treating at the mall today zoo last y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@Ussk81 PMSL!!! I try not to stare but I can't...</td>\n",
       "      <td>pmsl i try not to stare but i can t help it li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Sc0rpi0n676 btw - is there a remote chance i ...</td>\n",
       "      <td>btw is there a remote chance i will see you later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1</td>\n",
       "      <td>My fav part = \"government says\" RT @cnnbrk: Ir...</td>\n",
       "      <td>my fav part government say rt iranian presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>1</td>\n",
       "      <td>@arditord bro do you know? I got an urgent inf...</td>\n",
       "      <td>bro do you know i got an urgent info from our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>1</td>\n",
       "      <td>@senjohnmccain Are you for or against a govern...</td>\n",
       "      <td>are you for or against a government run plan w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>1</td>\n",
       "      <td>Rachel Maddow &amp; The Republican \"Obama Birther ...</td>\n",
       "      <td>rachel maddow the republican obama birther caucus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0</td>\n",
       "      <td>Have our H.S. &amp; colleges failed us? Part of th...</td>\n",
       "      <td>have our h s college failed u part of the st y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4004 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                              Tweet  \\\n",
       "0             0  Bumping dj sefs mixtape nowww this is my music...   \n",
       "1             0  #ieroween THE STORY OF IEROWEEN! THE VIDEO ->>...   \n",
       "2             0  trick or treating at the mall today; ZOO! last...   \n",
       "3             0  @Ussk81 PMSL!!! I try not to stare but I can't...   \n",
       "4             0  @Sc0rpi0n676 btw - is there a remote chance i ...   \n",
       "...         ...                                                ...   \n",
       "1999          1  My fav part = \"government says\" RT @cnnbrk: Ir...   \n",
       "2000          1  @arditord bro do you know? I got an urgent inf...   \n",
       "2001          1  @senjohnmccain Are you for or against a govern...   \n",
       "2002          1  Rachel Maddow & The Republican \"Obama Birther ...   \n",
       "2003          0  Have our H.S. & colleges failed us? Part of th...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "0     bumping dj sefs mixtape nowww this is my music...  \n",
       "1     ieroween the story of ieroween the video just ...  \n",
       "2     trick or treating at the mall today zoo last y...  \n",
       "3     pmsl i try not to stare but i can t help it li...  \n",
       "4     btw is there a remote chance i will see you later  \n",
       "...                                                 ...  \n",
       "1999  my fav part government say rt iranian presiden...  \n",
       "2000  bro do you know i got an urgent info from our ...  \n",
       "2001  are you for or against a government run plan w...  \n",
       "2002  rachel maddow the republican obama birther caucus  \n",
       "2003  have our h s college failed u part of the st y...  \n",
       "\n",
       "[4004 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the function on df by using the map function linked with the lambda function\n",
    "sentiment_copy['clean_tweet'] = sentiment_copy['Tweet'].map(lambda s: clean(s, step_list))\n",
    "\n",
    "# review dataframe\n",
    "sentiment_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38942ac5-33e0-44e9-be2e-7936e6ff303f",
   "metadata": {},
   "source": [
    "**Using the map(lambda s:...) method was a trick I had not known previously. This is a tool I plan on reusing in the future as before I had only known the .apply() method. Using the .apply() method with a previous version on the clean function was drawing errors as I was performing actions on the entire series as opposed to the individual string rows themselves.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12419060-6f99-47d7-8639-45f116e3dd80",
   "metadata": {},
   "source": [
    "**The regular tweets can be seen correctly transformed into their cleaned version in the 3rd column of our dataframe. All of the punctuation, twitter usernames and http links have been removed. Everything is in lowercase and the white spaces have been clipped. I wonder if there is another way further stem the values as I can see the word 'nowww' in the first row hasn't been changed to simply a 'now' stem. Further, the word H.S. turned into h and the word us turned into u. This u could be interpretted as us or you depending on how someone is tweeting. Dealing with this could improve our predictive capabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fdc92a1-098b-435f-9616-e2c00297479e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment       int64\n",
       "Tweet          object\n",
       "clean_tweet    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_copy.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c058d-7344-41f3-8858-bc4b74589502",
   "metadata": {},
   "source": [
    "**Double checking the object type**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0239fd-6844-4034-84c7-14638b57ca22",
   "metadata": {},
   "source": [
    "### 5. Use TfidfVectorizer from sklearn to prepare the data for machine learning.  Use max_features = 50;  <span style=\"color:red\" float:right>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad9af7f1-dd8b-40e5-a1aa-06ee1486981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>afghanistan</th>\n",
       "      <th>all</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>at</th>\n",
       "      <th>be</th>\n",
       "      <th>but</th>\n",
       "      <th>by</th>\n",
       "      <th>can</th>\n",
       "      <th>...</th>\n",
       "      <th>time</th>\n",
       "      <th>to</th>\n",
       "      <th>up</th>\n",
       "      <th>wa</th>\n",
       "      <th>we</th>\n",
       "      <th>what</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.768417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.440014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.24137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488726</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337813</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.331115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368618</td>\n",
       "      <td>0.356051</td>\n",
       "      <td>0.370403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253437</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4004 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  afghanistan  all  and       are        at   be       but   by  \\\n",
       "0       0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
       "1       0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
       "2       0.0          0.0  0.0  0.0  0.000000  0.347524  0.0  0.000000  0.0   \n",
       "3       0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.440014  0.0   \n",
       "4       0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
       "...     ...          ...  ...  ...       ...       ...  ...       ...  ...   \n",
       "3999    0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
       "4000    0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
       "4001    0.0          0.0  0.0  0.0  0.331115  0.000000  0.0  0.000000  0.0   \n",
       "4002    0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
       "4003    0.0          0.0  0.0  0.0  0.000000  0.000000  0.0  0.000000  0.0   \n",
       "\n",
       "           can  ...  time       to   up   wa        we      what      will  \\\n",
       "0     0.000000  ...   0.0  0.00000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "1     0.000000  ...   0.0  0.00000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "2     0.000000  ...   0.0  0.00000  0.0  0.0  0.768417  0.000000  0.000000   \n",
       "3     0.458611  ...   0.0  0.24137  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "4     0.000000  ...   0.0  0.00000  0.0  0.0  0.000000  0.000000  0.714283   \n",
       "...        ...  ...   ...      ...  ...  ...       ...       ...       ...   \n",
       "3999  0.000000  ...   0.0  0.00000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "4000  0.000000  ...   0.0  0.00000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "4001  0.000000  ...   0.0  0.00000  0.0  0.0  0.368618  0.356051  0.370403   \n",
       "4002  0.000000  ...   0.0  0.00000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "4003  0.000000  ...   0.0  0.00000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "          with       you  your  \n",
       "0     0.000000  0.000000   0.0  \n",
       "1     0.000000  0.000000   0.0  \n",
       "2     0.000000  0.000000   0.0  \n",
       "3     0.000000  0.000000   0.0  \n",
       "4     0.000000  0.488726   0.0  \n",
       "...        ...       ...   ...  \n",
       "3999  0.390681  0.000000   0.0  \n",
       "4000  0.000000  0.337813   0.0  \n",
       "4001  0.000000  0.253437   0.0  \n",
       "4002  0.000000  0.000000   0.0  \n",
       "4003  0.000000  0.000000   0.0  \n",
       "\n",
       "[4004 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create instance of TfidVectorizer and apply it to the clean tweets column\n",
    "clean_texts = sentiment_copy['clean_tweet']\n",
    "\n",
    "# create a tfidVectorizer instance\n",
    "vectorizer = TfidfVectorizer(max_features = 50)\n",
    "\n",
    "# fit and transform our clean texts to a matrix\n",
    "tfidf_matrix =  vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "# extract the column names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# change the original matrix to a dense array\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# create and review dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns = feature_names)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974830c-9303-488f-ba16-473e420a0f98",
   "metadata": {},
   "source": [
    "**We still have all of our rows from before fitting indicating the data has been preserved. The words have all been turned into their term frequency inverse document frequency values making their values of importance and usable in a predictive model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbadd0-2adb-495c-9c85-22736c8fa3a4",
   "metadata": {},
   "source": [
    "### 6. Use sklearn LogisticRegression to train a model on the  results on 75% of the data. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "### 7. determine the accuracy on the training data and the test data.   Determine the baseline accuracy. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "### 8. Repeat steps 5, 6, and 7  with TfidfVectorizer max_features set to 5, 500, 5000, 50000 and discuss your accuracies. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "293b529c-bde0-47ca-ba3e-7ceb0005fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets, features are the tfidf_df\n",
    "y_targets = sentiment_copy['Sentiment']\n",
    "\n",
    "# train test split the data into 75, 25 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, y_targets, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adc503-a51c-415c-923c-8ad27efbb63d",
   "metadata": {},
   "source": [
    "**The TF-IDF dataframe already has the engineered features we need so all that I need to do is grab the sentiment values as our targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa69cf9-11dc-46b3-8f4d-455fee76987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (3003, 50)\n"
     ]
    }
   ],
   "source": [
    "# review shape of training data\n",
    "print(type(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb560cd-b773-4a85-85df-bc03dbed49c2",
   "metadata": {},
   "source": [
    "**We can see that of the 4004 rows, the training data contains 3003 which is 75%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9fa63e4-05bf-4ad0-96d8-a60eebd90923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression instance\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# fit model to training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict results for both training and testing data\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf7af21-d732-459a-a8ad-13cb8aaa1a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7888777888777889\n",
      "Test accuracy: 0.8091908091908092\n",
      "Baseline accuracy: 0.5934065934065934\n"
     ]
    }
   ],
   "source": [
    "# determine accuracy for testing and training data and baseline accuracy\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34eeec-4f92-44be-aec6-e81cfbeadafa",
   "metadata": {},
   "source": [
    "**This first model with a max_feature parameter set to 50 garners a fairly decent predictive model. The test accuracy is at ~81% which tells us it is not overfitting the data, but is still giving us a high degree of accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62e8c0f9-6e9f-4903-8ed4-44e107596334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       594\n",
      "           1       0.81      0.69      0.75       407\n",
      "\n",
      "    accuracy                           0.81      1001\n",
      "   macro avg       0.81      0.79      0.80      1001\n",
      "weighted avg       0.81      0.81      0.81      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn.metrics to determine accuracy\n",
    "print(classification_report(y_test, test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865de21e-8eae-4b71-8e9f-d6a48fbe23e1",
   "metadata": {},
   "source": [
    "**With the max features set to 50 we are able to get a semi-good predictive ability for our algorithm with the accuracy being ~%78. It seems that having 50 features does a decent job of giving enough information for predicting sentiment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c6b1cf-9247-4286-9b2b-f97c9865364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_size': 50, 'accuracy': 0.809}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a running list of dictionaries to hold the feature size and accuracy\n",
    "accuracy_dict = [{'feature_size': 50, 'accuracy': test_acc.round(3)}]\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29bd85-7d25-4a82-ae80-5348ffd3ed1a",
   "metadata": {},
   "source": [
    "### Re do with max features set to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bf821f5-dbf0-410e-a31a-4f9f4a1d6c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>rt</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819869</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.742665</td>\n",
       "      <td>0.669663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582150</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4004 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      and        of        rt       the   to\n",
       "0     0.0  0.000000  0.000000  0.000000  0.0\n",
       "1     0.0  0.572551  0.000000  0.819869  0.0\n",
       "2     0.0  0.000000  0.000000  1.000000  0.0\n",
       "3     0.0  0.000000  0.000000  0.000000  1.0\n",
       "4     0.0  0.000000  0.000000  0.000000  0.0\n",
       "...   ...       ...       ...       ...  ...\n",
       "3999  0.0  0.742665  0.669663  0.000000  0.0\n",
       "4000  0.0  0.000000  0.000000  0.000000  0.0\n",
       "4001  0.0  0.000000  0.000000  0.000000  0.0\n",
       "4002  0.0  0.000000  0.000000  1.000000  0.0\n",
       "4003  0.0  0.813081  0.000000  0.582150  0.0\n",
       "\n",
       "[4004 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create instance of TfidVectorizer and apply it to the clean tweets column\n",
    "clean_texts = sentiment_copy['clean_tweet']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 5)\n",
    "tfidf_matrix =  vectorizer.fit_transform(clean_texts)\n",
    "# doc = 0\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# create and review dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns = feature_names)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58e418-14da-46ca-a4a1-ce95f4949282",
   "metadata": {},
   "source": [
    "**This dataframe looks entirely too simple to be of use to our sentiment prediction as the words presented are almost all conjunctions or prepositions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "915a30a2-29f5-4bcc-afcb-4f46355eaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets, features are the tfidf_df\n",
    "y_targets = sentiment_copy['Sentiment']\n",
    "\n",
    "# train test split the data into 75, 25 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, y_targets, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "708deb68-31bd-4696-be62-1abb6bb2d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (3003, 5)\n"
     ]
    }
   ],
   "source": [
    "# review shape of training data\n",
    "print(type(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4cd3131-2f34-48e3-8225-bf613d3f2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression instance\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# fit model to training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict results for both training and testing data\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "990bfd74-abca-4083-858d-e2b42bb755cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6117216117216118\n",
      "Test accuracy: 0.6103896103896104\n",
      "Baseline accuracy: 0.5934065934065934\n"
     ]
    }
   ],
   "source": [
    "# determine accuracy for testing and training data and baseline accuracy\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad1acf-0d33-4c9b-883b-b2c5d49e967a",
   "metadata": {},
   "source": [
    "**With only 5 features we have more or less a useless model that is correct around 60% of the time. This is performance is horrendous.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4c50d68-a861-4a75-9359-e35e5a7632a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67       594\n",
      "           1       0.52      0.53      0.53       407\n",
      "\n",
      "    accuracy                           0.61      1001\n",
      "   macro avg       0.60      0.60      0.60      1001\n",
      "weighted avg       0.61      0.61      0.61      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn.metrics to determine accuracy\n",
    "print(classification_report(y_test, test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e62c442f-0fc6-40bf-9513-faf15f8368c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_size': 50, 'accuracy': 0.809},\n",
       " {'feature_size': 5, 'accuracy': 0.61}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continue to add to our running list of dictionaries\n",
    "accuracy_dict.append({'feature_size': 5, 'accuracy': test_acc.round(3)})\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ba2bd-2722-47d0-95a6-0cb147fc0c31",
   "metadata": {},
   "source": [
    "### Re do with max features set to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dfe6b75-7599-47ec-ba77-7eb06d6e0ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actually</th>\n",
       "      <th>afghanistan</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>against</th>\n",
       "      <th>all</th>\n",
       "      <th>already</th>\n",
       "      <th>...</th>\n",
       "      <th>www</th>\n",
       "      <th>ya</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154331</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174837</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4004 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      about  act  action  actually  afghanistan  after  again   against  all  \\\n",
       "0       0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "1       0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "2       0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "3       0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "4       0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "...     ...  ...     ...       ...          ...    ...    ...       ...  ...   \n",
       "3999    0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "4000    0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "4001    0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.357807  0.0   \n",
       "4002    0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "4003    0.0  0.0     0.0       0.0          0.0    0.0    0.0  0.000000  0.0   \n",
       "\n",
       "      already  ...  www   ya  yeah      year  yes  yet   yo  york       you  \\\n",
       "0         0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.000000   \n",
       "1         0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.000000   \n",
       "2         0.0  ...  0.0  0.0   0.0  0.532491  0.0  0.0  0.0   0.0  0.000000   \n",
       "3         0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.000000   \n",
       "4         0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.321852   \n",
       "...       ...  ...  ...  ...   ...       ...  ...  ...  ...   ...       ...   \n",
       "3999      0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.000000   \n",
       "4000      0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.154331   \n",
       "4001      0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.174837   \n",
       "4002      0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.000000   \n",
       "4003      0.0  ...  0.0  0.0   0.0  0.000000  0.0  0.0  0.0   0.0  0.000000   \n",
       "\n",
       "      your  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "...    ...  \n",
       "3999   0.0  \n",
       "4000   0.0  \n",
       "4001   0.0  \n",
       "4002   0.0  \n",
       "4003   0.0  \n",
       "\n",
       "[4004 rows x 500 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create instance of TfidVectorizer and apply it to the clean tweets column\n",
    "clean_texts = sentiment_copy['clean_tweet']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 500)\n",
    "tfidf_matrix =  vectorizer.fit_transform(clean_texts)\n",
    "# doc = 0\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# create and review dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns = feature_names)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272b041-7605-4b17-8b7a-940bb4650438",
   "metadata": {},
   "source": [
    "**This data frame appears to be the most reasonable as the words displayed seem relevent, yet the number of features is deep enough to add value to our predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb3ec1ea-2c22-412d-a3ab-146671b0ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets, features are the tfidf_df\n",
    "y_targets = sentiment_copy['Sentiment']\n",
    "\n",
    "# train test split the data into 75, 25 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, y_targets, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eedd5d4e-ce7a-4244-b855-4cd391041f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (3003, 500)\n"
     ]
    }
   ],
   "source": [
    "# review shape of training data\n",
    "print(type(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "720ce799-65dc-4490-8a1f-13f5896f102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression instance\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# fit model to training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict results for both training and testing data\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "743a32f9-9f77-482b-86bf-f9ae49d0d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9054279054279054\n",
      "Test accuracy: 0.8861138861138861\n",
      "Baseline accuracy: 0.5934065934065934\n"
     ]
    }
   ],
   "source": [
    "# determine accuracy for testing and training data and baseline accuracy\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4462a-138e-4baf-bd49-10b6b340a682",
   "metadata": {},
   "source": [
    "**500 features is giving us an even higher accuracy than the 50 feature max. I believe we are approaching the overfitting realm, but not in it yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bf31895-cf50-4b3f-a4d9-5ffb54d90e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91       594\n",
      "           1       0.88      0.83      0.86       407\n",
      "\n",
      "    accuracy                           0.89      1001\n",
      "   macro avg       0.89      0.88      0.88      1001\n",
      "weighted avg       0.89      0.89      0.89      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn.metrics to determine accuracy\n",
    "print(classification_report(y_test, test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6278b9b9-5840-4f3f-ba30-3ef244c3a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_size': 50, 'accuracy': 0.809},\n",
       " {'feature_size': 5, 'accuracy': 0.61},\n",
       " {'feature_size': 500, 'accuracy': 0.886}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict.append({'feature_size': 500, 'accuracy':test_acc.round(3)})\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d61ab-be85-489a-a46e-19d262a8b839",
   "metadata": {},
   "source": [
    "### Re do with max features set to 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26b13377-431a-4d47-a1ed-a3269c3a18c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaah</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aan</th>\n",
       "      <th>aarp</th>\n",
       "      <th>ab</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>abroad</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yu</th>\n",
       "      <th>zero</th>\n",
       "      <th>zijn</th>\n",
       "      <th>zippity</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4004 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaaaaaaaaaaaaaaaaaaaaah  aaaah  aan  aarp   ab  able  abortion  about  \\\n",
       "0                         0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "1                         0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "2                         0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "3                         0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "4                         0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "...                       ...    ...  ...   ...  ...   ...       ...    ...   \n",
       "3999                      0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "4000                      0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "4001                      0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "4002                      0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "4003                      0.0    0.0  0.0   0.0  0.0   0.0       0.0    0.0   \n",
       "\n",
       "      above  abroad  ...  youth  youtube       yr   yu  zero  zijn  zippity  \\\n",
       "0       0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "1       0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "2       0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "3       0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "4       0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "...     ...     ...  ...    ...      ...      ...  ...   ...   ...      ...   \n",
       "3999    0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "4000    0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "4001    0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "4002    0.0     0.0  ...    0.0      0.0  0.00000  0.0   0.0   0.0      0.0   \n",
       "4003    0.0     0.0  ...    0.0      0.0  0.29694  0.0   0.0   0.0      0.0   \n",
       "\n",
       "      zombie    zoo   zu  \n",
       "0        0.0  0.000  0.0  \n",
       "1        0.0  0.000  0.0  \n",
       "2        0.0  0.301  0.0  \n",
       "3        0.0  0.000  0.0  \n",
       "4        0.0  0.000  0.0  \n",
       "...      ...    ...  ...  \n",
       "3999     0.0  0.000  0.0  \n",
       "4000     0.0  0.000  0.0  \n",
       "4001     0.0  0.000  0.0  \n",
       "4002     0.0  0.000  0.0  \n",
       "4003     0.0  0.000  0.0  \n",
       "\n",
       "[4004 rows x 5000 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create instance of TfidVectorizer and apply it to the clean tweets column\n",
    "clean_texts = sentiment_copy['clean_tweet']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "tfidf_matrix =  vectorizer.fit_transform(clean_texts)\n",
    "# doc = 0\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# create and review dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns = feature_names)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b596ba1-dcf7-4422-959e-e71c18410558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets, features are the tfidf_df\n",
    "y_targets = sentiment_copy['Sentiment']\n",
    "\n",
    "# train test split the data into 75, 25 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, y_targets, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76900126-f3f3-4039-a86a-bcc24827208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (3003, 5000)\n"
     ]
    }
   ],
   "source": [
    "# review shape of training data\n",
    "print(type(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c922b203-80e5-4a8c-8c0f-3a7a8a8fe4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression instance\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# fit model to training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict results for both training and testing data\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c06f027b-a45a-4ae1-950e-ae35dcab90b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.939060939060939\n",
      "Test accuracy: 0.8971028971028971\n",
      "Baseline accuracy: 0.5934065934065934\n"
     ]
    }
   ],
   "source": [
    "# determine accuracy for testing and training data and baseline accuracy\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da44d22-e62b-440f-ade5-fe1ccba9e326",
   "metadata": {},
   "source": [
    "**The test accuracy did not improve very much from the 500 feature max to the 5000 feature max. Both were in the high 80% range.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3da2f2d2-8366-4c72-872e-dcb2dad761a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       594\n",
      "           1       0.90      0.84      0.87       407\n",
      "\n",
      "    accuracy                           0.90      1001\n",
      "   macro avg       0.90      0.89      0.89      1001\n",
      "weighted avg       0.90      0.90      0.90      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn.metrics to determine accuracy\n",
    "print(classification_report(y_test, test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc099be1-3a39-4ffc-b33b-92760b22a14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_size': 50, 'accuracy': 0.809},\n",
       " {'feature_size': 5, 'accuracy': 0.61},\n",
       " {'feature_size': 500, 'accuracy': 0.886},\n",
       " {'feature_size': 5000, 'accuracy': 0.897}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict.append({'feature_size': 5000, 'accuracy':test_acc.round(3)})\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a523ec1-8e63-4be5-934a-4fef34d4c651",
   "metadata": {},
   "source": [
    "### Re do with max features set to 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44a5577e-2290-4649-b1c6-5da4857242ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaah</th>\n",
       "      <th>aaaaaahhhhhhh</th>\n",
       "      <th>aaaah</th>\n",
       "      <th>aagghh</th>\n",
       "      <th>aahh</th>\n",
       "      <th>aan</th>\n",
       "      <th>aanslag</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aarp</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zombified</th>\n",
       "      <th>zona</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zorgen</th>\n",
       "      <th>zou</th>\n",
       "      <th>zshare</th>\n",
       "      <th>zu</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4004 rows × 10549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aaaaaaaaaaaaaaaaaaaaaah  aaaaaahhhhhhh  aaaah  aagghh  aahh  aan  \\\n",
       "0                         0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "1                         0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "2                         0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "3                         0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "4                         0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "...                       ...            ...    ...     ...   ...  ...   \n",
       "3999                      0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "4000                      0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "4001                      0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "4002                      0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "4003                      0.0            0.0    0.0     0.0   0.0  0.0   \n",
       "\n",
       "      aanslag  aaron  aarp   ab  ...  zombified  zona  zone  zoning    zoo  \\\n",
       "0         0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "1         0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "2         0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.301   \n",
       "3         0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "4         0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "...       ...    ...   ...  ...  ...        ...   ...   ...     ...    ...   \n",
       "3999      0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "4000      0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "4001      0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "4002      0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "4003      0.0    0.0   0.0  0.0  ...        0.0   0.0   0.0     0.0  0.000   \n",
       "\n",
       "      zorgen  zou  zshare   zu   zz  \n",
       "0        0.0  0.0     0.0  0.0  0.0  \n",
       "1        0.0  0.0     0.0  0.0  0.0  \n",
       "2        0.0  0.0     0.0  0.0  0.0  \n",
       "3        0.0  0.0     0.0  0.0  0.0  \n",
       "4        0.0  0.0     0.0  0.0  0.0  \n",
       "...      ...  ...     ...  ...  ...  \n",
       "3999     0.0  0.0     0.0  0.0  0.0  \n",
       "4000     0.0  0.0     0.0  0.0  0.0  \n",
       "4001     0.0  0.0     0.0  0.0  0.0  \n",
       "4002     0.0  0.0     0.0  0.0  0.0  \n",
       "4003     0.0  0.0     0.0  0.0  0.0  \n",
       "\n",
       "[4004 rows x 10549 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create instance of TfidVectorizer and apply it to the clean tweets column\n",
    "clean_texts = sentiment_copy['clean_tweet']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 50000)\n",
    "tfidf_matrix =  vectorizer.fit_transform(clean_texts)\n",
    "# doc = 0\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# create and review dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns = feature_names)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f40adb7d-a734-4c6d-b64b-532ae0d52522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets, features are the tfidf_df\n",
    "y_targets = sentiment_copy['Sentiment']\n",
    "\n",
    "# train test split the data into 75, 25 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, y_targets, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fb39892-d2f9-4831-b2e6-2bf853fd49f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (3003, 10549)\n"
     ]
    }
   ],
   "source": [
    "# review shape of training data\n",
    "print(type(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "233cb84d-16dc-4f79-b203-1f80341303ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression instance\n",
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "# fit model to training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict results for both training and testing data\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f208718-dfe0-43b0-8d3b-27979ce17775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9473859473859474\n",
      "Test accuracy: 0.8951048951048951\n",
      "Baseline accuracy: 0.5934065934065934\n"
     ]
    }
   ],
   "source": [
    "# determine accuracy for testing and training data and baseline accuracy\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3c05f-0db2-4b9d-8cb4-92eafe8a6f07",
   "metadata": {},
   "source": [
    "**Performance seems to taper off at this point where the max features are set to 50000. However, the training accuracy is overfitting the data at this point. We may want to use a lower feature max for our predictive model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536da71-7469-4341-b05c-de71fe6cd6ad",
   "metadata": {},
   "source": [
    "**With such a high number of features we start to enter the realm of overfitting with accuracies getting into the 90%. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bba1b60-87dd-4c10-8e90-c2800b91a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91       594\n",
      "           1       0.90      0.83      0.87       407\n",
      "\n",
      "    accuracy                           0.90      1001\n",
      "   macro avg       0.90      0.89      0.89      1001\n",
      "weighted avg       0.90      0.90      0.89      1001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use sklearn.metrics to determine accuracy\n",
    "print(classification_report(y_test, test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8d395ae-46ae-43f0-a300-55eb90cbeaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature_size': 50, 'accuracy': 0.809},\n",
       " {'feature_size': 5, 'accuracy': 0.61},\n",
       " {'feature_size': 500, 'accuracy': 0.886},\n",
       " {'feature_size': 5000, 'accuracy': 0.897},\n",
       " {'feature_size': 50000, 'accuracy': 0.895}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append new values to dictionary\n",
    "accuracy_dict.append({'feature_size': 50000, 'accuracy':test_acc.round(3)})\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1468b127-0cab-4f34-be98-765b0a4851aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TestAccuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_size</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>0.895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TestAccuracy\n",
       "feature_size              \n",
       "5                    0.610\n",
       "50                   0.809\n",
       "500                  0.886\n",
       "5000                 0.897\n",
       "50000                0.895"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe of accuracy and feature size\n",
    "accuracy_df = pd.DataFrame(accuracy_dict)\n",
    "accuracy_df = accuracy_df.set_index('feature_size')\n",
    "accuracy_df.columns = ['TestAccuracy']\n",
    "\n",
    "# display df\n",
    "accuracy_df = accuracy_df.sort_index(ascending=True)\n",
    "\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcbc6214-20ce-4068-9964-0f49bba5050e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Test Accuracy vs Feature Size'}, xlabel='Log(Feature Size)', ylabel='Percentage (%)'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot on a graph, but take the log of the feature size and convert TestAccuracy to percent\n",
    "x = np.log(list(accuracy_df.index))\n",
    "y = np.array(accuracy_df['TestAccuracy'])*100\n",
    "\n",
    "plt.axes(title='Test Accuracy vs Feature Size', xlabel='Log(Feature Size)', ylabel='Percentage (%)')\n",
    "sns.lineplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c6edd-8510-4895-8960-e17692dce139",
   "metadata": {},
   "source": [
    "**Here we can see the trend in accuracy as the feature size increases. Predictive test accuracy increases with feature size, almost exponentially. But after a certain point the accuracy flattens out and even drops a little.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91922dad-4bd6-4e3e-972f-45a527e78e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
