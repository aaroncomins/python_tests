{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we return to the topic of feature engineering from the previous lecture, but look at more complex examples of feature engineering. Many of these examples combine **domain knowledge** with machine learning. What this means is that the kind of data we have lends itself to certain types of features engineering that may not apply to other types of data. The best way to learn about this kind of feature engineering is by looking at examples: feature engineering with text data, with time series data, with image or video data, with sound data, and so on. So as our example, we will look at feature engineering for text data. To make things more interesting, we finish this notebook by training a supervised learning algorithm on the resulting data. This last part will warm us up for next week's lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example dealing with text data. Raw text data is an unstructured and ubiquitous type of data. Most of the world’s data is unstructured. Volumes of unstructured data, including text, are growing much faster than structured data. There are many industry estimates for the fraction of all data which is unstructured. How much text data are we talking about here? In a few years time, Twitter will have [more text data recorded](http://www.internetlivestats.com/twitter-statistics/) than all that has been written in print in the history of mankind.\n",
    "\n",
    "Given the ubiquity and volume of text data, it is not surprising that numerous powerful applications which exploit text analytics are appearing. A few of these applications are listed below.\n",
    "\n",
    "- Intelligent applications\n",
    "  - Assistants\n",
    "  - Chat bots\n",
    "- Classification\n",
    "  - Sentiment analysis\n",
    "  - SPAM detection\n",
    "- Speech recognition\n",
    "- Search\n",
    "- Information retrieval\n",
    "- Legal discovery\n",
    "\n",
    "In this tutorial we investigate two areas of text analytics:\n",
    "\n",
    "- Pre-processing text data for analysis\n",
    "- Classification of text and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import coo_matrix # this is the sparse matrix format discussed in lecture\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP) in Python\n",
    "- The main NLP package in Python is nltk\n",
    "- You may need to download some accessory packages like 'wordnet' or 'stopwords'\n",
    "\n",
    "Uncomment the following four nltk.download statements if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# # A one-time requirement for these four downloads:\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('corpus')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first step to prepare text is to clean it.  We clean and normalize the text by performing various operations on the text. Some examples are as follows:\n",
    "\n",
    "- Make the text lowercase.\n",
    "- Remove symbols or punctuation.\n",
    "- Remove numbers. May also replace all numbers with a numeric tag, for example `<NUM>` or similar. We may also consider replacing all dates with `<DATE>` or similarly use tags `<URL>`, `<PHONE>`, `<EMAIL>`, etc...\n",
    "- Strip extra white space. White space has many forms: space, newline, or tab. There are also other rarely used unicode specifications for other white space characters.\n",
    "- Remove all non-printable unicode characters.\n",
    "- Replace accent characters.\n",
    "- Remove 'stop words'. Stop words are generally non-informative words like \"the\", \"as\", \"a\", etc.\n",
    "- Stem words to similar endings, such as \"eats\" and \"eat\".\n",
    "\n",
    "There are a few reasons to clean your text.  The primary reason is to reduce the potential vocabulary and increase the observations of specific words (or tokens). Depending on the application, the above steps should be considered carefully and only applied when it makes sense. Ask yourself if words like \"China\" and \"china\" should be different.\n",
    "\n",
    "**NOTE**: Be careful dealing with unicode characters. There are many editors and text viewers that only display printable characters but will not remove non-printable characters. Strange unicode characters can end up in data from users blindly copy/pasting text (with invisible unicode) into other text boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (8 minutes)\n",
    "\n",
    "By its very nature, text data comes unstructured and poorly organized for analysis. Typically multiple steps are required to process text into a form suitable for analysis, starting with cleaning it.\n",
    "\n",
    "Here's a horrible tweet. We're going to clean it and learn about ways to clean text data in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered text: \n",
      "I <3 cleaning data $\\ \\ $, it’s my ၲ  $\\ \\ $    fAvoRitEs!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horrible_tweet_text = 'I <3 cleaning data $\\ \\ $, it’s my \\u1072  $\\ \\ $    fAvoRitEs!! 11!!!'\n",
    "print('Unfiltered text: \\n{}\\n'.format(horrible_tweet_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step, refine the above text. To avoid overwriting the existing string, you can create a new string from the string so far each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-ASCII Characters\n",
    "- A string `x` is ASCII if `ord(x) < 128`.  \n",
    "\n",
    "For instance:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P is #80; ၲ is #4210; 科 is #31185\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u0050 is #{}; \\u1072 is #{}; \\u79d1 is #{}\".format(ord('\\u0050'), ord('\\u1072'), ord('\\u79d1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any non-ASCI character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 2 Non-Ascii characters: ’ၲ\n",
      "The tweet has 63  ascii characters, namely:\n",
      " I <3 cleaning data $\\ \\ $, its my   $\\ \\ $    fAvoRitEs!! 11!!!\n"
     ]
    }
   ],
   "source": [
    "# Add code here to remove non-ASCII characters\n",
    "\n",
    "# 1\n",
    "# def GoodAscii(x):\n",
    "#     if ord(x) < 128:\n",
    "#         return x\n",
    "#     return ''\n",
    "# AsciiString = ''.join(list(map(GoodAscii, horrible_tweet_text)))\n",
    "\n",
    "# 2\n",
    "AsciiString = ''.join([x for x in horrible_tweet_text if ord(x) < 128])\n",
    "\n",
    "# 3\n",
    "# AsciiString = ''.join(filter(lambda x: x in string.printable, horrible_tweet_text))\n",
    "\n",
    "# 4\n",
    "# AsciiString = ''\n",
    "# for char in horrible_tweet_text: \n",
    "#     if ord(char) < 128: \n",
    "#         AsciiString = AsciiString + char\n",
    "\n",
    "# Find all of the non-ascii characters\n",
    "# NonAsciiString = horrible_tweet_text.translate({ord(i): None for i in set(string.printable)})\n",
    "NonAsciiString = ''.join([x for x in horrible_tweet_text if ord(x) >= 128])\n",
    "\n",
    "print(\"We found\", len(NonAsciiString), \"Non-Ascii characters:\", NonAsciiString)\n",
    "print(\"The tweet has\", len(AsciiString), \" ascii characters, namely:\\n\", AsciiString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Lowercase\n",
    "- Make all the letters lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String in lowercase:\n",
      " i <3 cleaning data $\\ \\ $, its my   $\\ \\ $    favorites!! 11!!!\n"
     ]
    }
   ],
   "source": [
    "lowerString = AsciiString.lower()\n",
    "print(\"String in lowercase:\\n\", lowerString) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove Punctuation\n",
    "- Remove all punctuation from the text. You can use `string.punctuation` to get a string of all punctuation characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String without punctuation:\n",
      " i 3 cleaning data    its my         favorites 11\n"
     ]
    }
   ],
   "source": [
    "# Add code here\n",
    "\n",
    "# 1\n",
    "NonPunctString = \"\".join([x for x in lowerString if x not in string.punctuation])\n",
    "\n",
    "# 2\n",
    "# NonPunctString = lowerString.translate({ord(i): None for i in string.punctuation})\n",
    "\n",
    "# 3\n",
    "# NonPunctString = lowerString.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# 4\n",
    "# NonPunctString = re.sub(r'[^\\w\\s]', '', lowerString)\n",
    "\n",
    "print(\"String without punctuation:\\n\", NonPunctString) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Numbers\n",
    "- Remove all the numbers from the text. HINT: If you are familiar with **regular expressions** you can use `re.sub` to do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String without numbers:\n",
      " i  cleaning data    its my         favorites \n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "NoNumberString = re.sub(\"[0-9]\", \"\", NonPunctString)\n",
    "\n",
    "# 2\n",
    "# NoNumberString = ''.join([x for x in NonPunctString if not x.isdigit()])\n",
    "\n",
    "print(\"String without numbers:\\n\", NoNumberString) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim Whitespace\n",
    "- Strip the text of any extra whitespace. HINT: The `split` method might help here.  Using `split` creates a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i cleaning data its my favorites\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "Tokens = NoNumberString.split()\n",
    "TrimmedString = \" \".join(Tokens)\n",
    "\n",
    "# 2\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# Tokens = nltk.word_tokenize(NoNumberString) # nltk\n",
    "# TrimmedString = \" \".join(Tokens)\n",
    "\n",
    "# 3\n",
    "# replace whitespaces between words with single blanks\n",
    "# TrimmedString = re.sub(r'\\s+', ' ', NoNumberString)\n",
    "\n",
    "print(TrimmedString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords\n",
    "- Remove all stop words from the text. We can use `stopwords.words('english')` to get a list of stop words for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_334/4172322119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTrimmedTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrimmedString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNoStopWordsTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTrimmedTokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mNoStopWordsString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNoStopWordsTokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNoStopWordsString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_334/4172322119.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTrimmedTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrimmedString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNoStopWordsTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTrimmedTokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mNoStopWordsString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNoStopWordsTokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNoStopWordsString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jovyan/nltk_data'\n    - '/opt/conda/nltk_data'\n    - '/opt/conda/share/nltk_data'\n    - '/opt/conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "TrimmedTokens = TrimmedString.split()\n",
    "NoStopWordsTokens = [word for word in TrimmedTokens if not word in stopwords.words('english')]\n",
    "NoStopWordsString = \" \".join([x for x in NoStopWordsTokens])\n",
    "print(NoStopWordsString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize vs. Stemming\n",
    "- Here is an example of how words can be changed by lemmatization:\n",
    "    - rocks : rock\n",
    "    - corpora : corpus\n",
    "    - better : good\n",
    "- Here is an example of how words are changed by traditional stemming\n",
    "    - rocks : rock\n",
    "    - corpora : corp\n",
    "    - better : better\n",
    "- Let's now use `WordNetLemmatizer()` to reduce the words to their **stems**. HINT: Use the object's `lemmatize` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoStopWordsTokens = NoStopWordsString.split()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "LematizedTokens = [lemmatizer.lemmatize(word) for word in NoStopWordsTokens]\n",
    "print(\"Lematized Tokens:\", LematizedTokens)\n",
    "\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "StemmedTokens = [stemmer.stem(word) for word in NoStopWordsTokens]\n",
    "print(\"Snowball-Stemmed Tokens:\", StemmedTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine the steps outlined in the above exercise and create a function to clean text for use. We will later use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_stopwords':\n",
    "            stops = stopwords.words('english')\n",
    "            word_list = text.split(' ')\n",
    "            text_words = [word for word in word_list if word not in stops]\n",
    "            text = ' '.join(text_words)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "step_list = ['remove_non_ascii', 'lowercase', 'remove_punctuation', 'remove_numbers',\n",
    "            'strip_whitespace', 'remove_stopwords', 'stem_words']\n",
    "\n",
    "print(\"before: \\\"{}\\\"\".format(horrible_tweet_text))\n",
    "print(\"after : \\\"{}\\\"\".format(preprocess(horrible_tweet_text, step_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order Matters\n",
    "- Why do the following preprocess commands have different effects?  \n",
    "- Can you think of any other examples where the order of preprocessing text matters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = 'we took statistics200'\n",
    "\n",
    "print('first stem then remove numbers:', preprocess(my_string, ['stem_words', 'remove_numbers']))\n",
    "print('first remove numbers then stem:', preprocess(my_string, ['remove_numbers', 'stem_words']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizing text data\n",
    "\n",
    "We the clean data we can begin to ask what is the best way to extract features from the data. There are many more approaches for text analytics and natural language processing (NLP). We only mention a few below. Note that the collection of unique words in the data is called a **corpus**. To avoid having a corpus that's too large, we can trim the corpus by keeping the most frequent $N$ words, making $N$ the size of the corpus. A **document** usually refers to a single data point with raw text, such as a tweet, a review, an invoice, etc. So our documents are made up of \"words\" that come from the corpus (ignoring any words that are not in the corpus). The question now is how do we represent such a data numerically? Here are two approaches:\n",
    "\n",
    "- The **bag of words model** is a simple and surprisingly effective model for analysis of text data. The BOW model creates a **sparse vector representation** of each word in the corpus based on the frequency of the words in the document. The order of the words is not considered, nor is the similarity between different words. Despite serious shortcomings, the model can work well in many cases.\n",
    "- We can usually do much better by using **word embeddings**, which are **dense vector respresentations** for each word in the corpus. Word embeddings are learned by examining the word's **context** (other words around it). Word embeddings are very common in **deep learning** applications of NLP, although the embeddings themselves are learned using a shallow network. If we learn word embeddings from a very large data set once, we can save and re-use these word embeddings to create features for other data sets. In fact, **pre-trained word embeddings** are trained by large companies like Google and made available for use by others. So we can load these embeddings and numerically represent a document using the average of the embeddings of the words in it. Because word embeddings are vectors, such an average would also be a vector that is a dense representation of the document.\n",
    "\n",
    "As you can see, BOW models seem too simplistic and word embeddings seem too sophisticated for now. So here's another approach that is sort of between the two in terms of difficulty. It is called TF-IDF and it is a clever way to featurize words in documents. Just like a BOW model, we begin by \"tokenizing\" the data. In BOW we then create a one-hot encoded feature for each token (or word). But in TF-IDF we first extract the relative word frequencies per document (called **term frequencies** or TF), we then multiply the term frequencies by a multiplier we call IDF. This has the effect of dampening the values for terms that appear frequently across documents, giving them less influence when we move on to the machine learning phase. Note that we used the words \"token\", \"word\" and \"term\" almost interchangeably. Sorry for confusing you! Data scientists don't always agree on terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text\n",
    "\n",
    "As a first step in preparing text for analysis of a document is to **tokenize** the text. In general terms, tokenization is the process dividing raw text into words, symbols and other elements, known as **tokens**. A set of tokens from all documents in the data is known as a **corpus**.\n",
    "\n",
    "As a first step in creating a corpus is reading the data set. This particular data set is comprised of 160,000 tweets. The sentiment of these tweets has been human labeled as positive or negative (4 is for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/twitter_data.csv'\n",
    "tweet_df = pd.read_csv(data_file)[0:50000]\n",
    "print(tweet_df.shape)\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data set read, we need to clean then tokenize the tweets. Note that stemming can be slow on large datasets.  It's sometimes helpful to profile such functions to find the main culprits and see if we can do anything to speed them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace', 'stem_words']\n",
    "#tweet_df['clean_tweet'] = tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))\n",
    "cProfile.run(\"tweet_df['clean_tweet'] = tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To help us understand TF-IDF in great detail, we learn about TF-IDF in two different ways:\n",
    "\n",
    "- We first run through the steps \"manually\", so that we can see talk about implementation details. \n",
    "- We then apply TF-IDF using `sklearn` in a few lines of code, which will hide all of that detail.\n",
    "\n",
    "## Computing TF-IDF\n",
    "\n",
    "Computing the TF-IDF values manually consist of several steps:\n",
    "1. We obtain the term-document matirx. \n",
    "1. We calculate the **term-frequency (TF)** matirx, where $\\text{TF(doc, term)}$ is the term frequencies from the term document matrix divided by the total number of terms in the document. In other words, we turn the frequencies into percentages (also called relative frequencies).\n",
    "1. We trim the term-document matrix if needed. This gets rid of useless words that frequently appear across documents.\n",
    "1. We can also derive the **inverse document frequencies (IDF)** matrix:\n",
    "\n",
    "   $$\\text{IDF(term)} = \\log(\\frac{\\text{number of documents + 1}}{\\text{number of documents with term in it} + 1})$$\n",
    "\n",
    "   Note that TF is a function of both term and document, but the IDF is just a function of the terms.\n",
    "1. We multiply the TF with IDF values to get TF-IDF values. This results in **term frequency inverse document frequency (TF-IDF) maxtrix**. \n",
    "   \n",
    "   $$\\text{TF-IDF} = \\text{TF(term, doc)} \\cdot \\text{IDF(term)}$$\n",
    "\n",
    "To get an intuition behind what's happening, here's what the IDF function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "x = np.linspace(0, 100, num=101)\n",
    "sns.lineplot(x = x, y = np.log((100 + 1) / (x + 1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFs give us the relative frequency terms in the documents. We use the IDFs to then dampen the TFs for terms that appear frequently across documents by virtue of being common terms. This will make it so rarer terms get to exert more influence during the machine learning phase.\n",
    "\n",
    "Applications of TF-IDF include\n",
    "\n",
    "- Characterize writing styles\n",
    "- Comparing authors\n",
    "- Determining original authors\n",
    "- Finding plagiarism\n",
    "\n",
    "Let's now implement this for the tweet data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get the term-document matrix\n",
    "\n",
    "![](https://library.startlearninglabs.uw.edu/DATASCI410/img/tdm.png)\n",
    "We first break up the words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = tweet_df['clean_tweet']\n",
    "docs = {}\n",
    "for ix, row in enumerate(clean_texts):\n",
    "    docs[ix] = row.split(' ')\n",
    "\n",
    "print('Example entry: {}'.format(docs[np.random.choice(ix)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfWords = 0\n",
    "for word_list in docs.values():\n",
    "    numberOfWords += len(word_list)\n",
    "print('Our cleaned tweet corpus has', numberOfWords, 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our document-term matrix, keeping track of its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nonzero = 0\n",
    "vocab = set()\n",
    "\n",
    "for word_list in docs.values():\n",
    "    unique_terms = set(word_list)    # all unique terms of this tweet\n",
    "    vocab.update(unique_terms)       # set union: add unique terms of this tweet\n",
    "    num_nonzero += len(unique_terms) # add count of unique terms in this tweet for the COO sparse matrix\n",
    "\n",
    "print('Our tweet-vocabulary has {} distinct words.'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert everything to a numpy array. We should keep track of how the vocab/term indices map to the matrix so that we can look them up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(list(vocab))\n",
    "print('Some example vocabulary: {}'.format(vocab[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize the **document-term matrix**. This is a matrix where each row $i$ is a document and each column $j$ is a word. The entriy $(i, j)$ of the matrix shows the frequency of the word $j$ in document $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's allow indexing on the vocabulary \n",
    "vocab_sorter = np.argsort(vocab)\n",
    "# doc_key_list = np.array(list(docs.keys()))\n",
    "vocab_size = len(vocab)\n",
    "data = np.empty(num_nonzero, dtype = np.intc)     # all non-zero\n",
    "rows = np.empty(num_nonzero, dtype = np.intc)     # row index\n",
    "cols = np.empty(num_nonzero, dtype = np.intc)     # column index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now populate the document-term matrix (DTM). Each row $i$ is a document and each column $j$ is a word. The entry $(i, j)$ of the matrix shows the frequency of the word $j$ in document $i$.  <br/><br/> In our code:\n",
    "- rows contain the document number\n",
    "- cols contains the word index\n",
    "- data contains the number of times a given word appeared in a given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "# go through all documents with their terms\n",
    "print('Computing, please wait!')\n",
    "for doc_key, terms in docs.items():\n",
    "    # find indices to insert-into such that, if the corresponding elements were\n",
    "    # inserted before the indices, the order would be preserved\n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter = vocab_sorter)]\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts = True)\n",
    "    n_vals = len(uniq_indices)  # number of unique terms\n",
    "    ix_end = ix + n_vals # add count to index\n",
    "    data[ix:ix_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ix:ix_end] = uniq_indices            # save the column index: index in \n",
    "    # doc_ix = np.where(doc_key_list == doc_key)   # get the document index for the document name\n",
    "    #rows[ix:ix_end] = np.repeat(doc_ix, n_vals)  # save it as repeated value\n",
    "    rows[ix:ix_end] = np.repeat(doc_key, n_vals)  # save it as repeated value\n",
    "    ix = ix_end  # resume with next document -> will add future data on the end\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rows refers to documents {} through {}'.format(min(rows), max(rows)))\n",
    "print('cols refers to token indices {} through {}'.format(min(cols), max(cols)))\n",
    "print('data indicates that a term may occur from {} to {} times in a document'.format(min(data), max(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our sorted vocabulary to see examples of words that we may not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorted Vocab: {}'.format(vocab[vocab_sorter[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we probably need to do some trimming, as the word 'aaaaa' probably doesn't occur often enough, and having over 60000 unique words may be too much.  We will address this later on. For now, let's keep the corpus we have. We now construct a COO sparse matrix for the document-term matrix encoded by `rows`, `cols`, and `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_mat = coo_matrix((data, (rows, cols)), shape = (tweet_df.shape[0], vocab_size), dtype = np.intc)\n",
    "doc_term_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the binary sentiment label.\n",
    "# The sentiment label has values 4 and 0.\n",
    "# Typically, binary labels have values 1 and 0.\n",
    "# Replace label \"4\" with label \"1\":\n",
    "tweet_df['sentiment_label'] = tweet_df['sentiment_label'].replace(4, 1)\n",
    "\n",
    "# Convert a copy of the tweets as list for use later\n",
    "tweet_data = tweet_df.values.tolist()\n",
    "print(tweet_df['sentiment_label'].value_counts())\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We populated the matrix with the term frequencies. We can see an example of this for the word \"python\" in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check to make sure!\n",
    "vocab_list = list(vocab)\n",
    "word_of_interest = 'python'\n",
    "vocab_interesting_ix = list(vocab).index(word_of_interest)\n",
    "print('vocab index of {} : {}'.format(word_of_interest, vocab_interesting_ix))\n",
    "# find which tweets contain word\n",
    "doc_ix_with_word = []\n",
    "for ix, row in enumerate(tweet_data): # note on this line later\n",
    "    if word_of_interest in row[1]:\n",
    "        doc_ix_with_word.append(ix)\n",
    "\n",
    "print('\\n1st document index containing said word: {}'.format(doc_ix_with_word[0]))\n",
    "print('\\nTweet: {}'.format(tweet_data[doc_ix_with_word[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word \"python\" appears once in the presented document (tweet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The term-document matirx `doc_term_mat` is a sparse matirx. So we can't index it using row and column index. Here we don't really need to do that, but if we did we would first need to convert it into an array. We can use the `toarray` method to do that, but if the matrix is large we can easily run out of memory (that's why we're using a spares matrix in the first place!). So instead we can use the `tocsr` method shown below, which uses compression to avoid the memory problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document-term matrix relevant entry\n",
    "document_row = doc_ix_with_word[0]\n",
    "vocab_col = vocab_interesting_ix\n",
    "mat_entry = doc_term_mat.tocsr()[document_row, vocab_col]\n",
    "\n",
    "print('\\nRow {} and column {} of document-term matrix has entry {}'.format(document_row, vocab_col, mat_entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Trimming the document-term matrix\n",
    "\n",
    "We saw above that we are including terms like 'aaaaa' and 'aaaa', which probably occur very few times. These terms generally occur with unstructured text fields because we allow users to input whatever they feel like and that includes typos.  But be aware that they can also be artifacts of our cleaning process (unintentionally and intentionally).\n",
    "\n",
    "Since our document-term matrix is a matrix of counts of words (columns) in each document (rows), we want to remove words that don't occur very frequently across our corpus. The count of how frequent a word is in all of our corpus is just the sum of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = doc_term_mat.sum(axis = 0)\n",
    "display(pd.DataFrame(word_counts).T.value_counts()[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many words are above a specific cutoff, such as 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 15\n",
    "word_count_list = word_counts.tolist()[0]\n",
    "col_cutoff_ix = [ix for ix, count in enumerate(word_count_list) if count > cutoff]\n",
    "\n",
    "print('Number of words w/counts above {} : {}'.format(cutoff, len(col_cutoff_ix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now trim our vocabulary and document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_trimmed = np.array([vocab[x] for x in col_cutoff_ix])\n",
    "vocab_sorter_trimmed = np.argsort(vocab_trimmed)\n",
    "\n",
    "print('Shape of document-term matrix before trimming: {}'.format(doc_term_mat.shape))\n",
    "\n",
    "doc_term_mat_trimmed = doc_term_mat.tocsc()[:, col_cutoff_ix]\n",
    "print('Shape of document-term matrix after trimming: {}'.format(doc_term_mat_trimmed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first 10 words alphabetically\n",
    "vocab_trimmed[vocab_sorter_trimmed[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what cutoff we should use? Let's look at a bar plot of the first 20 words in descending frequency before and after we trimmed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.value_counts(word_count_list, normalize = True)[:20]\n",
    "words = [vocab[x] for x in counts.index][:20]\n",
    "g = sns.barplot(x = words, y = counts, color = 'lightblue')\n",
    "g.set_xticklabels(rotation = 90, labels = words);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the bar plot is very skewed. There are too many words that appear few times. Let's check out a 20-word sample from the trimmed list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_word_counts = doc_term_mat_trimmed.sum(axis = 0)\n",
    "trimmed_word_list = trimmed_word_counts.tolist()[0]\n",
    "\n",
    "counts = pd.value_counts(trimmed_word_list, normalize = True)[:20]\n",
    "# counts = counts.reset_index(drop = True)\n",
    "words = [vocab[x] for x in counts.index][:20]\n",
    "g = sns.barplot(x = words, y = counts, color = 'lightblue')\n",
    "g.set_xticklabels(rotation = 90, labels = words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of words for each appearance\n",
    "hist_breaks = np.arange(0, 500, 5)\n",
    "plt.hist(word_count_list, bins = hist_breaks)\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Histogram of word counts from 1 to 500')\n",
    "plt.show()\n",
    "\n",
    "# Count of words for each appearance\n",
    "hist_breaks = np.arange(0, 30, 1)\n",
    "plt.hist(word_count_list, bins = hist_breaks)\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Histogram of word counts from 1 to 30')\n",
    "plt.show()\n",
    "\n",
    "# Too many words appear few times. Check out trimmed.\n",
    "trimmed_word_counts = doc_term_mat_trimmed.sum(axis=0)\n",
    "trimmed_word_list = trimmed_word_counts.tolist()[0]\n",
    "hist_breaks = np.arange(0, 500, 5)\n",
    "plt.hist(trimmed_word_list, bins = hist_breaks)\n",
    "plt.xlabel('Number of Documents')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Histogram of trimmed word counts for less than 500')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 15 might be a good cutoff, but these types of **hyperparameters** for the model will probably need tuning. We will learn about tuning such values in future lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compute IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we compute IDF values using the following formula:\n",
    "\n",
    "$$\\text{IDF(term)} = \\log(\\frac{\\text{number of documents}}{\\text{number of documents with term in it}})$$\n",
    "\n",
    "The advantage of using a linear algebra library like numpy is that we can easily implement this in a few lines of code without having to write loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_mat_trimmed = doc_term_mat_trimmed / (doc_term_mat_trimmed.sum(axis = 1) + 0.0000000000001)\n",
    "# vocab_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_mat_trimmed = doc_term_mat_trimmed.sum(axis = 0) # This should be axis = 0\n",
    "idf_mat_trimmed = np.log((doc_term_mat_trimmed.shape[0] + 1) / (idf_mat_trimmed + 1))\n",
    "idf_mat_trimmed = idf_mat_trimmed.reshape(-1).transpose()\n",
    "idf_mat_trimmed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compute TF-IDF\n",
    "\n",
    "Finally, we can now compute the TF-IDF vaules by just multiplying the TF and IDF arrays. Let's first check their dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF has shape {} and IDF has shape {}\".format(tf_mat_trimmed.shape, idf_mat_trimmed.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By \"multiplying\" the two arrays, we are NOT talking about matrix multiplication. What we do is multiply each column of the TF matrix (the terms) with its corresponding value of the IDF vector for that term. This multiplication can be very slow so we have the code commented out. In the next exercise, we re-implement TF-IDF but use the efficient `sklern` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf_mat = np.multiply(tf_mat_trimmed, csr_matrix(idf_mat_trimmed))\n",
    "# tf_idf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (5 minutes)\n",
    "\n",
    "Unfortunately, our `numpy` implementation of TF-IDF would be very slow even on this smallish dataset. So we commented out the code above. But there is a much more efficient implementation of TF-IDF in `sklearn` and it does all of the computation steps behind the scenes. All we need to do is pass it the tokenized data. So in practice, we can use the `TfidfVectorizer` function to do all the calculations for us. \n",
    "\n",
    "- Take a moment to familiarize yourself with this function and use it to get TF-IDF features from the tweets that we cleaned earlier. Notice the similarities between this function and similar functions in `sklearn` we learned eariler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_texts = tweet_df['clean_tweet']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 500)\n",
    "tfidf_matrix =  vectorizer.fit_transform(clean_texts)\n",
    "doc = 0\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix_dense, columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_df.shape)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'csr' (compressed sparse row) format is used by `sklearn` to store the matrix.\n",
    "\n",
    "### End of exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Classification and sentiment analysis\n",
    "\n",
    "It's hard to talk about feature engineering without moving on to the next step: training a machine learning model. Although we cover this topic on a future lesson, we include here an example in case you can't wait any longer!\n",
    "\n",
    "Now that we have a prepared DTM of the 50000 tweets, let's build and evaluate models to classify the sentiment of these tweets. The idea is simple: We use the TF-IDF features for training the model. Since our data also has a column that says if the tweet expresses a positive or negative sentiment, we will train a model to predict the sentiment from the TF-IDF features. So first let's obtain the TF-IDF features once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 6228, stop_words = 'english')\n",
    "clean_texts = tweet_df['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tf_idf_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the featurized data into training and test sets. We will explain why in a future lecture. For training we will use 70% to 90% tweets to predict the 0,1 sentiment. The remaining cases will be used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targets = np.array([y[0] for y in tweet_data])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 5000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a logistic classifier on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the precision, recall and Fscore of the model for positive and negative tweets.\n",
    "\n",
    "Recall that a positive prediction here means a positive review, and so **precision** is the proportion of correct predictions among all positive predictions and **recall** is the proportion of correct predictions among all true positives. **F1** is the harmonic average of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##  Summary\n",
    "\n",
    "NLP applications extend far and wide, so we only stratched the surface here. Many of the modern breakthroughs in deep learning for example have been in NLP. One reason for this is that language data is abundant and the lack of structure in the data presents us with many challenges and learning opportunities. We hope this notebook exposed you to just some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "1. use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy:\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "<br/> <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "2. concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "3. replace sentiment labels 'POLIT': 1, 'NOT': 0; <span style=\"color:red\" float:right>[0 point]</span>\n",
    "\n",
    "4. clean the tweets\n",
    "   1. remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "   2. remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "   3. **replace** (not remove) all punctuation marks with a space (\" \")\n",
    "   4. **replace** all numbers with a space\n",
    "   5. **replace** all non ascii characters with a space\n",
    "   7. convert all characters to lowercase\n",
    "   8. strip extra whitespaces\n",
    "   9. lemmatize tokens\n",
    "   9. No need to remove stopwords because TfidfVectorizer will take care of that\n",
    "<br/><span style=\"color:red\" float:right>[9 point]</span>\n",
    "\n",
    "5. Use TfidfVectorizer from sklearn to prepare the data for machine learning.  Use max_features = 50;  <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n",
    "6. Use sklearn LogisticRegression to train a model on the  results on 75% of the data. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "7. determine the accuracy on the training data and the test data.   Determine the baseline accuracy. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "8. Repeat steps 5, 6, and 7  with TfidfVectorizer max_features set to 5, 500, 5000, 50000 and discuss your accuracies. <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n",
    "# End of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
